{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note** :  Le code étant le même pour chaque maladie à part l'équation de recherche, le même code sera utilisé plusieurs fois en changeant juste l'équation de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping de la première page de résultat avec ajout d'un test pour assurer la bonne récupération des mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de recherche sur PubMed avec l'équation de recherche\n",
    "query = \"(sleep habits OR sleep patterns OR sleep quality) AND (Alzheimer's disease OR cognitive decline OR dementia)\"  #Alzheimer\n",
    "url = f\"https://pubmed.ncbi.nlm.nih.gov/?term={query.replace(' ', '+')}\"\n",
    "\n",
    "# Ajout d'un User-Agent pour éviter le blocage\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Envoi de la requête\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # Recherche des articles dans la page\n",
    "    for result in soup.find_all(\"article\", class_=\"full-docsum\"):\n",
    "        title_tag = result.find(\"a\", class_=\"docsum-title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Titre inconnu\"\n",
    "        link = \"https://pubmed.ncbi.nlm.nih.gov\" + title_tag[\"href\"] if title_tag else \"Lien non disponible\"\n",
    "\n",
    "        # Initialisation des valeurs par défaut\n",
    "        publication_date = \"Date inconnue\"\n",
    "        summary = \"Résumé non disponible\"\n",
    "        keywords = \"Mots-clés non disponibles\"\n",
    "        authors = \"Auteurs non disponibles\"\n",
    "        affiliations = \"Affiliations non disponibles\"\n",
    "\n",
    "        if title_tag and title_tag[\"href\"]:\n",
    "            article_url = link\n",
    "            article_response = requests.get(article_url, headers=headers)\n",
    "\n",
    "            if article_response.status_code == 200:\n",
    "                article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                # Extraction de l'abstract et des mots-clés dans la div \"abstract\"\n",
    "                abstract_div = article_soup.find(\"div\", class_=\"abstract\")\n",
    "                if abstract_div:\n",
    "                    # Extraction du résumé\n",
    "                    summary_tag = abstract_div.find(\"div\", class_=\"abstract-content\")\n",
    "                    if summary_tag:\n",
    "                        paragraphs = summary_tag.find_all(\"p\")\n",
    "                        summary = \" \".join([p.text.strip() for p in paragraphs]) if paragraphs else summary_tag.text.strip()\n",
    "\n",
    "                    # Extraction des mots-clés avec une approche plus flexible\n",
    "                    for p in abstract_div.find_all(\"p\"):\n",
    "                        strong_tag = p.find(\"strong\", class_=\"sub-title\")\n",
    "                        if strong_tag and \"Keywords\" in strong_tag.text:  # Vérifie si \"Keywords\" est présent dans le texte\n",
    "                            keywords = p.text.replace(\"Keywords:\", \"\").strip()\n",
    "                            print(f\" Mots-clés extraits: {keywords}\")\n",
    "                            break  # On arrête dès qu'on trouve les mots-clés\n",
    "\n",
    "\n",
    "                # Extraction de la date de publication\n",
    "                date_tag = article_soup.find(\"span\", class_=\"cit\") or article_soup.find(\"time\", class_=\"history-date\")\n",
    "                if date_tag:\n",
    "                    publication_date = date_tag.text.strip()\n",
    "\n",
    "                # Extraction des auteurs\n",
    "                authors_tag = article_soup.find(\"div\", class_=\"authors-list\")\n",
    "                if authors_tag:\n",
    "                    authors = \", \".join([a.text.strip() for a in authors_tag.find_all(\"a\")])\n",
    "\n",
    "                # Extraction des affiliations\n",
    "                affiliations_tag = article_soup.find(\"div\", class_=\"affiliations\")\n",
    "                if affiliations_tag:\n",
    "                    affiliations = \"; \".join([aff.text.strip() for aff in affiliations_tag.find_all(\"li\")])\n",
    "\n",
    "            \n",
    "            time.sleep(2)   # Pause pour éviter le blocage\n",
    "\n",
    "        articles.append([title, publication_date, link, summary, keywords, authors, affiliations])\n",
    "\n",
    "    # Création du dataFrame\n",
    "    df = pd.DataFrame(articles, columns=[\"Titre\", \"Date de publication\", \"Lien\", \"Résumé\", \"Mots-clés\", \"Auteurs\", \"Affiliations\"])\n",
    "\n",
    "    # Sauvegarde en CSV\n",
    "    df.to_csv(\"pubmed_articles_scraped.csv\", index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête : {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
